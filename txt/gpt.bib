@article{gptreport,
  title={Improving language understanding with unsupervised learning},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018},
  publisher={Technical report, OpenAI},
  url={https://openai.com/research/language-unsupervised}
}

@article{gpt,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@misc{transformer,
  doi = {10.48550/ARXIV.1706.03762},
  url = {https://arxiv.org/abs/1706.03762},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Attention Is All You Need},
  publisher = {arXiv},
  year = {2017}
}

@article{weaksupervision,
  doi = {10.48550/ARXIV.2010.07835},
  url = {https://arxiv.org/abs/2010.07835},
  author = {Yu, Yue and Zuo, Simiao and Jiang, Haoming and Ren, Wendi and Zhao, Tuo and Zhang, Chao},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach},
  publisher = {arXiv},
  year = {2020}
}

@misc{finetuninghuman,
  doi = {10.48550/ARXIV.1909.08593},
  url = {https://arxiv.org/abs/1909.08593},
  author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  title = {Fine-Tuning Language Models from Human Preferences},
  publisher = {arXiv},
  year = {2019}
}

@mastersthesis{arneba,
  title={Automatische {G}enerierung komplexer {F}ragen zum Informationsmanagement auf der Basis der {SNIK}-Ontologie},
  author={Arne Roszeitis},
  type={Bachelor's Thesis},
  school={Institute for Medical Informatics, Statistics {and} Epidemiology (IMISE)},
  address={Leipzig, Germany},
  url={https://www.snik.eu/public/bachelor-ar.pdf},
  year={2022}
}

@mastersthesis{hannesbell,
  title={{Q}uestion {A}nswering auf {SNIK}},
  author={Hannes Raphael Brunsch},
  type={Besondere {L}ernleistung},
  school={Wilhelm-Ostwald-Schule},
  address={Leipzig, Germany},
  url={https://www.snik.eu/public/bell-hrb.pdf},
  year={2022}
}

@article{pretraining,
  title={Don't stop pretraining: Adapt language models to domains and tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  journal={arXiv preprint arXiv:2004.10964},
  year={2020}
}
